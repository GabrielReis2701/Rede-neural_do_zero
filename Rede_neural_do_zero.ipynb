{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Rede-neural-do-zero.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOQ85R+mQt0HtbNrYS82jtl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GabrielReis2701/Rede-neural_do_zero/blob/main/Rede_neural_do_zero.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OJ2csd_Q-_G6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "from time import time\n",
        "from torchvision import datasets, transforms\n",
        "from torch import nn, optim"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.ToTensor() #definindo a conversão de imagem para tensor\n",
        "\n",
        "trainset = datasets.MNIST('./MNIST_data/', download=True, train=True, transform=transform) # Carrega a parte de treino do dataset\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True) # Cria um buffer para pegar os dados por partes\n",
        "valset = datasets.MNIST('./MNIST_data/', download=True, train=False, transform=transform) # Carrega a parte de validação do dataset\n",
        "valloader = torch.utils.data.DataLoader(valset, batch_size=64, shuffle=True) # Cria um buffer para pegar os dados por partes"
      ],
      "metadata": {
        "id": "BuQ7iBBQ__6T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataiter = iter(trainloader)\n",
        "imagens, etiquetas = dataiter.next()\n",
        "plt.imshow(imagens[0].numpy().squeeze(), cmap='gray_r');"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "AI0bHGyyB4Wb",
        "outputId": "578cbfe3-3b9d-4f10-c189-9d6ef4a6e093"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(imagens[0].shape) #Para verificar as dimensões do tensor de cada imagem\n",
        "print(etiquetas[0].shape) #Para verificar as dimensões do tensor de cada etiqueta"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7qNKrxqXCcFt",
        "outputId": "16286c86-38d9-48c4-8424-482307b0d370"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 28, 28])\n",
            "torch.Size([])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Modelo(nn.Module):\n",
        "    def _init_(self):\n",
        "      super(Modelo, self)._init_()\n",
        "      self.linear1 = nn.Linear(28*28, 128) # Camada de entrada, 784 neurônios que se ligam a 128\n",
        "      self.linear2 = nn.Linear(128, 64) # Camada interna 1, 128 neurônios que se ligam a 64\n",
        "      self.linear3 = nn.Linear(64, 10) # Camada interna 2, 64 neurônios que se ligam a 10\n",
        "      #Para a camada de saida não e necessario definir nada pois só é preciso pegar o output da camada interna 2\n",
        "\n",
        "    def forward(self,x):\n",
        "      x = F.relu(self.linear1(x)) # função de ativação da camada de entrada para a camada interna 1\n",
        "      x = F.relu(self.linear2(x)) # função de ativação da camada interna 1 para a camada interna 2\n",
        "      x = self.linear3(x) # função de ativação da camada interna 2 para a camada de saida, nesse caso f(x) = x\n",
        "      return F.log_softmax(x, dim=1) # dados utilizados para calcular a perda"
      ],
      "metadata": {
        "id": "tBXqzAWkCzct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def treino(modelo, trainloader, device):\n",
        "  otimizador = optim.SGD(modelo.parameters(), lr=0.01, momentum=0.5) # define a politica de atualização dos pesos e da bias\n",
        "  inicio = time() # timer para saber quanto tempo levou o treino\n",
        "  criterio = nn.NLLLoss() # definindo o criterio para calcular a perda\n",
        "  EPOCHS = 100 # numero de epochs que p algoritmo rodará\n",
        "  modelo.train() # ativando o modo de treinamento do modelo\n",
        "  for epoch in range(EPOCHS):\n",
        "    perda_acumulada =0 # inicialização da perda acumulada da epoch em questão\n",
        "    for imagens, etiquetas in trainloader:\n",
        "      imagens = imagens.view(imagens.shape[0], -1) # convertendo as imagens para vetores de 28*28 casas\n",
        "      otimizador.zero_grad() # zerando os gradientes por conta do ciclo anterior\n",
        "      output = modelo(imagens.to(device)) # colocando os dados no modelo\n",
        "      perda_instantanea = criterio(output, etiquetas.to(device)) # calculando a perda da epoch em questão\n",
        "      perda_instantanea.backward() # back propagation a partir da perda\n",
        "      otimizador.step() # atualizando os pesos e a bias\n",
        "      perda_acumulada += perda_instantanea.item() # atualização da perda acumulada\n",
        "    else:\n",
        "      print(\"Epoch {} - perda resultante: {}\".format(epoch+1, perda_acumulada/len(trainloader)))\n",
        "  print(\"\\nTempo de treino (em minutos) = \",(time()-inicio)/60)"
      ],
      "metadata": {
        "id": "64Wmev9BHET8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validacao(modelo, valloader, device):\n",
        "  conta_corretas, conta_todas = 0, 0\n",
        "  for imagens,etiquetas in valloader:\n",
        "    for i in range(len(etiquetas)):\n",
        "      img = imagens[i].view(1, 784)\n",
        "      #desativar o autograd para acelerar a validação. Grafos computacionais dinâmicos tem um custo alto de processamento\n",
        "      with torch.no_grad():\n",
        "        logps = modelo(img.to(device)) # output do modelo em escala logaritmica\n",
        "      ps = torch.exp(logps) # converte output para escala normal( lembrando que é um tensor)\n",
        "      probab = list(ps.cpu().numpy()[0])\n",
        "      etiqueta_pred = probab.index(max(probab)) # converte o tensor em um número, no caso, o numero que o modelo previu\n",
        "      etiqueta_certa = etiquetas.numpy()[i]\n",
        "      if(etiqueta_certa == etiqueta_pred):  #compara a previsão com o valor correto\n",
        "        conta_corretas += 1\n",
        "      conta_todas += 1\n",
        "  print(\"Total de imagens testadas =\", conta_todas)\n",
        "  print(\"\\nPrecisão do modelo = {} %\".format((conta_corretas*100)/conta_todas))"
      ],
      "metadata": {
        "id": "cBKGM1Q3MKgE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelo = Modelo()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "modelo.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KsItPsOBMv5b",
        "outputId": "e9bb06bd-d61b-4625-88ed-c19b1002f383"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Modelo()"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    }
  ]
}